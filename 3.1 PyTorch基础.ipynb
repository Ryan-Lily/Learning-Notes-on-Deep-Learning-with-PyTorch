{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chapter1 《Tensor张量：PyTorch里面处理的最基本的操作对象》\n",
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "#Tensor默认的数据类型为：torch.FloatTensor\n",
    "a = torch.Tensor([[2, 3],.1 [4, 8], [7, 9]])\n",
    "print('a is: {}'.format(a))\n",
    "\n",
    "#torch.LongTensor数据类型的Tensor\n",
    "b = torch.LongTensor([[2, 3], [4, 8], [7, 9]])\n",
    "print('b is: {}'.format(b))\n",
    "\n",
    "#全为0的Tensor\n",
    "c = torch.zeros((3, 2))\n",
    "print('zero tensor: {}'.format(c))\n",
    "\n",
    "#服从正态分布的Tensor\n",
    "d = torch.randn((3, 2))\n",
    "print('normal random is: {}'.format(d))\n",
    "\n",
    "#通过索引的方式进行赋值\n",
    "a[0, 1] = 100\n",
    "print('changed a is: {}'.format(a))\n",
    "\n",
    "#torch.Tensor和numpy.ndarray之间的相互转换\n",
    "numpy_b = b.numpy()    #tensor --> array\n",
    "print('conver to numpy is \\n {}'.format(numpy_b))\n",
    "\n",
    "e = np.array([[2, 3], [4, 5]])    #array --> tensor\n",
    "torch_e = torch.from_numpy(e)\n",
    "print('from numpy to torch.Tensor is: {}'.format(torch_e))\n",
    "\n",
    "#将tensor放到GPU上\n",
    "if torch.cuda.is_available():\n",
    "    a_cuda = a.cuda()\n",
    "    print(a_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chapter2 《Variable变量：1、神经网络计算图里特有的一个概念，2、提供了自动求导的功能，3、其在torch.autograd.Variable中，4、Variable有三个比较重要的组成属性：data、grad、grad_fn》\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#对标量求倒数\n",
    "x = Variable(torch.Tensor([1]), requires_grad = True)\n",
    "w = Variable(torch.Tensor([2]), requires_grad = True)\n",
    "b = Variable(torch.Tensor([3]), requires_grad = True)\n",
    "\n",
    "y = w * x +b\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n",
    "\n",
    "#对向量求倒数\n",
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad = True)\n",
    "\n",
    "y = 2 * x\n",
    "print(y)\n",
    "\n",
    "y.backward(torch.FloatTensor([1, 0.1, 0.01]))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在PyTorch里面编写神经网络，所有的层结构和损失函数都来自于torch.nn\n",
    "import torch.nn as nn\n",
    "\n",
    "#定义神经网络模型\n",
    "class net_name(nn.Module):\n",
    "    def __init__(self, other_arguments):\n",
    "        super(net_name, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "\n",
    "#定义损失函数\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#优化算法就是一种调整模型参数更新的策略，使得损失函数最小化。2、torch.optim是一个实现各种优化算法的包\n",
    "import torch.optim as optim\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型的保存与加载\n",
    "\n",
    "#保存和加载整个模型(结构信息和参数信息)\n",
    "torch.save(model, './model.pth')\n",
    "torch.load('model.pth')\n",
    "#保存和加载模型的参数信息\n",
    "torch.save(model.state_dict(), './model_state.pth')\n",
    "model.load_state_dic(torch.load('model_state.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
